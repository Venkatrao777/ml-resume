{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96fc604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://b168f952abb150be1a.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b168f952abb150be1a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/ml-resume/.mlenv/lib/python3.12/site-packages/gradio/queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/.mlenv/lib/python3.12/site-packages/gradio/route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/.mlenv/lib/python3.12/site-packages/gradio/blocks.py\", line 2250, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/.mlenv/lib/python3.12/site-packages/gradio/blocks.py\", line 1757, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/.mlenv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/.mlenv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/.mlenv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/.mlenv/lib/python3.12/site-packages/gradio/utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/functions.py\", line 137, in process_resume\n",
      "    response_string = get_resume_response(prompt, my_sk)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/functions.py\", line 103, in get_resume_response\n",
      "    response = client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/.mlenv/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/.mlenv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/.mlenv/lib/python3.12/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ml-resume/.mlenv/lib/python3.12/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - [{'error': {'code': 404, 'message': 'models/gpt-4o-mini is not found for API version v1main, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}]\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as app:\n",
    "    # create header and app description\n",
    "    gr.Markdown(\"# Resume Optimizer ðŸ“„\")\n",
    "    gr.Markdown(\"Upload your resume, paste the job description, and get actionable insights!\")\n",
    "\n",
    "    # gather inputs\n",
    "    with gr.Row():\n",
    "        resume_input = gr.File(label=\"Upload Your Resume (.md)\")    \n",
    "        jd_input = gr.Textbox(label=\"Paste the Job Description Here\", lines=9, interactive=True, placeholder=\"Paste job description...\")\n",
    "    run_button = gr.Button(\"Optimize Resume ðŸ¤–\")\n",
    "\n",
    "    # display outputs\n",
    "    output_resume_md = gr.Markdown(label=\"New Resume\")\n",
    "    output_suggestions = gr.Markdown(label=\"Suggestions\")\n",
    "\n",
    "    # editing results\n",
    "    output_resume = gr.Textbox(label=\"Edit resume and export!\", interactive=True)\n",
    "    export_button = gr.Button(\"Export Resume as PDF ðŸš€\")\n",
    "    export_result = gr.Markdown(label=\"Export Result\")\n",
    "    \n",
    "    # Event binding\n",
    "    run_button.click(process_resume, inputs=[resume_input, jd_input], outputs=[output_resume_md, output_resume, output_suggestions])\n",
    "    export_button.click(export_resume, inputs=[output_resume], outputs=[export_result])\n",
    "\n",
    "# Launch the app\n",
    "app.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
